{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgpQJHNSfsK6"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import face_detection \n",
    "from sklearn.cluster import DBSCAN\n",
    "from keras.models import load_model\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import tqdm\n",
    "import argparse\n",
    "# from imutils.video import FileVideoStream\n",
    "# from imutils.video import FPS\n",
    "# import imutils\n",
    "import time\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.11'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygki7lR_sFvf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparser = argparse.ArgumentParser(description=\"SocialDistancing\")\\nparser.add_argument(\"--videopath\", type=str, default=\"video1.mp4\", help=\"Path to the video file\")\\nargs = parser.parse_args()\\n\\nFILE_PATH = args.videopath\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the Working Environment\n",
    "\n",
    "# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n",
    "BASE_PATH = \"drive/My Drive/Social_Distancing_with_AI-master/\"\n",
    "\n",
    "# Path to Input Video File in the BASE_PATH\n",
    "FILE_PATH = \"video1.mp4\"\n",
    "# Command-line input setup\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description=\"SocialDistancing\")\n",
    "parser.add_argument(\"--videopath\", type=str, default=\"video1.mp4\", help=\"Path to the video file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "FILE_PATH = args.videopath\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "2fcfd62efc6b4cd1b0dd14b646254afe",
      "3c354ebd36324219bdcdf98c96b72721",
      "874d2f0343564a238d15b9d61d3bdeaf",
      "8910c4e06c0346b187525153761f2657",
      "a188a817c2bd43c5aa85e4ea7c848c66",
      "b33a91cd105f489abb9294f003f2ccbb",
      "edf7785d6c47473e95b8066c025de957",
      "f8acf0e98afb432b9bc3665dcb41dd98"
     ]
    },
    "colab_type": "code",
    "id": "2xrRSzoT9EBa",
    "outputId": "e6ae9db2-a205-459e-a437-495c11bd1616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DSFDDetector', 'RetinaNetResNet50', 'RetinaNetMobileNetV1']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Face Detector \n",
    "# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n",
    "\n",
    "print(face_detection.available_detectors)\n",
    "detector = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.5, nms_iou_threshold=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kv-5woacC0C5"
   },
   "outputs": [],
   "source": [
    "# Load Pretrained Face Mask Classfier (Keras Model)\n",
    "\n",
    "mask_classifier = load_model(\"Models/ResNet50_Classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8O6mF_orCxN4"
   },
   "outputs": [],
   "source": [
    "# Set the Safe Distance in Pixel Units (Minimum Distance Expected to be Maintained between People)\n",
    "# This Parameter would Affect the Results, Adjust according to the Footage captured by CCTV Camera \n",
    "\n",
    "threshold_distance = 150  # Try with different Values before Finalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ejHxuy-9iZwL",
    "outputId": "3091f973-51fc-44b8-8972-c57a792772c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frames :\n",
      "frame start process 24.3426569\n",
      "object detected 24.5694535\n",
      "face detected 26.1070799\n",
      "frame start process 27.4088324\n",
      "object detected 27.4101871\n",
      "face detected 27.6824951\n",
      "frame start process 27.8857075\n",
      "object detected 27.8871319\n",
      "face detected 28.1782156\n",
      "frame start process 28.4303052\n",
      "object detected 28.4319786\n",
      "face detected 28.7740265\n",
      "frame start process 29.115545\n",
      "object detected 29.1181084\n",
      "face detected 29.5992779\n",
      "frame start process 29.8314137\n",
      "object detected 29.8336367\n",
      "face detected 30.135626\n",
      "frame start process 30.3567314\n",
      "object detected 30.3583678\n",
      "face detected 30.6103967\n",
      "frame start process 30.82168\n",
      "object detected 30.8242242\n",
      "face detected 31.1154923\n",
      "frame start process 31.3233299\n",
      "object detected 31.32495\n",
      "face detected 31.6186137\n",
      "frame start process 31.8308665\n",
      "object detected 31.8328921\n",
      "face detected 32.1370523\n",
      "frame start process 32.3382855\n",
      "object detected 32.3400903\n",
      "face detected 32.6104127\n",
      "frame start process 32.8140539\n",
      "object detected 32.8157328\n",
      "face detected 33.0767645\n",
      "frame start process 33.2856769\n",
      "object detected 33.288158\n",
      "face detected 33.5533162\n",
      "frame start process 33.755754\n",
      "object detected 33.7572076\n",
      "face detected 34.0157115\n",
      "face detected 34.2616787\n",
      "frame start process 34.5516932\n",
      "object detected 34.5534593\n",
      "face detected 34.7986732\n",
      "face detected 35.0461251\n",
      "frame start process 35.2596645\n",
      "object detected 35.2610583\n",
      "face detected 35.5251767\n",
      "face detected 35.793311\n",
      "frame start process 35.9968885\n",
      "object detected 35.9984458\n",
      "face detected 36.2741869\n",
      "face detected 36.5284947\n",
      "frame start process 36.7461132\n",
      "object detected 36.7475882\n",
      "face detected 37.0021472\n",
      "face detected 37.2645782\n",
      "frame start process 37.4710963\n",
      "object detected 37.4725843\n",
      "face detected 37.7264736\n",
      "face detected 37.9803001\n",
      "frame start process 38.1922329\n",
      "object detected 38.1938211\n",
      "face detected 38.4544466\n",
      "face detected 38.7090292\n",
      "frame start process 38.9179046\n",
      "object detected 38.9192313\n",
      "face detected 39.1793384\n",
      "face detected 39.4328794\n",
      "frame start process 39.6412613\n",
      "object detected 39.6426778\n",
      "face detected 39.9030629\n",
      "face detected 40.2009482\n",
      "frame start process 40.4079584\n",
      "object detected 40.4097579\n",
      "face detected 40.6659962\n",
      "face detected 40.921533\n",
      "frame start process 41.133922\n",
      "object detected 41.1349316\n",
      "face detected 41.3944692\n",
      "face detected 41.6535967\n",
      "frame start process 41.8641452\n",
      "object detected 41.8654241\n",
      "face detected 42.1418596\n",
      "face detected 42.3965612\n",
      "frame start process 42.6128457\n",
      "object detected 42.6139344\n",
      "face detected 42.8749241\n",
      "face detected 43.1262051\n",
      "frame start process 43.3329067\n",
      "object detected 43.3347811\n",
      "face detected 43.5531271\n",
      "face detected 43.8113672\n",
      "frame start process 44.019888\n",
      "object detected 44.02141\n",
      "face detected 44.3045905\n",
      "face detected 44.6011386\n",
      "frame start process 44.8110786\n",
      "object detected 44.8130872\n",
      "face detected 45.2231958\n",
      "face detected 45.5073231\n",
      "frame start process 45.7294993\n",
      "object detected 45.7314156\n",
      "face detected 46.0776951\n",
      "face detected 46.2761451\n",
      "frame start process 46.4914013\n",
      "object detected 46.4930611\n",
      "face detected 46.7560628\n",
      "face detected 47.0282513\n",
      "frame start process 47.2420139\n",
      "object detected 47.2436991\n",
      "face detected 47.5072808\n",
      "face detected 47.7675229\n",
      "frame start process 47.969245\n",
      "object detected 47.9706861\n",
      "face detected 48.2304451\n",
      "face detected 48.4837486\n",
      "frame start process 48.6933335\n",
      "object detected 48.6948317\n",
      "face detected 48.9573789\n",
      "face detected 49.2182757\n",
      "frame start process 49.42448\n",
      "object detected 49.4264177\n",
      "face detected 49.689047\n",
      "face detected 49.9360484\n",
      "frame start process 50.147256\n",
      "object detected 50.1486722\n",
      "face detected 50.4282742\n",
      "face detected 50.6883137\n",
      "frame start process 50.8986033\n",
      "object detected 50.8999921\n",
      "face detected 51.1555609\n",
      "face detected 51.4084547\n",
      "frame start process 51.6345133\n",
      "object detected 51.6359026\n",
      "face detected 51.8905677\n",
      "face detected 52.1418193\n",
      "frame start process 52.3475825\n",
      "object detected 52.3491379\n",
      "face detected 52.6103546\n",
      "face detected 52.8567443\n",
      "frame start process 53.0708559\n",
      "object detected 53.0722363\n",
      "face detected 53.3436439\n",
      "face detected 53.5943391\n",
      "frame start process 53.8080976\n",
      "object detected 53.8096133\n",
      "face detected 54.0629695\n",
      "face detected 54.3218485\n",
      "frame start process 54.5292233\n",
      "object detected 54.5310846\n",
      "face detected 54.7817682\n",
      "face detected 55.0365839\n",
      "frame start process 55.2480874\n",
      "object detected 55.2494892\n",
      "face detected 55.5095006\n",
      "face detected 55.7631438\n",
      "frame start process 55.9702367\n",
      "object detected 55.9717592\n",
      "face detected 56.2402878\n",
      "face detected 56.4947356\n",
      "frame start process 56.703953\n",
      "object detected 56.7050452\n",
      "face detected 56.9641146\n",
      "face detected 57.2214386\n",
      "frame start process 57.4290533\n",
      "object detected 57.4308502\n",
      "face detected 57.6918449\n",
      "face detected 57.9392421\n",
      "frame start process 58.1555454\n",
      "object detected 58.1570955\n",
      "face detected 58.4223756\n",
      "face detected 58.6719004\n",
      "frame start process 58.890613\n",
      "object detected 58.8920315\n",
      "face detected 59.1515173\n",
      "face detected 59.4068003\n",
      "frame start process 59.6143799\n",
      "object detected 59.616028\n",
      "face detected 59.8755199\n",
      "face detected 60.1314709\n",
      "frame start process 60.3507763\n",
      "object detected 60.3524686\n",
      "face detected 60.7072724\n",
      "face detected 60.9952097\n",
      "frame start process 61.2115644\n",
      "object detected 61.2131367\n",
      "face detected 61.5531595\n",
      "face detected 61.8503096\n",
      "frame start process 62.0609936\n",
      "object detected 62.0624054\n",
      "face detected 62.3352116\n",
      "face detected 62.5865605\n",
      "frame start process 62.79545\n",
      "object detected 62.7965155\n",
      "face detected 63.0568681\n",
      "face detected 63.2576863\n",
      "frame start process 63.469439\n",
      "object detected 63.4708395\n",
      "face detected 63.7253725\n",
      "face detected 63.9875492\n",
      "frame start process 64.2003604\n",
      "object detected 64.2018652\n",
      "face detected 64.4571921\n",
      "face detected 64.7207004\n",
      "frame start process 64.9280662\n",
      "object detected 64.9297222\n",
      "face detected 65.1962075\n",
      "face detected 65.4487044\n",
      "frame start process 65.6554844\n",
      "object detected 65.656973\n",
      "face detected 65.9079103\n",
      "face detected 66.1774488\n",
      "frame start process 66.3849832\n",
      "object detected 66.3869075\n",
      "face detected 66.6482348\n",
      "face detected 66.900855\n",
      "frame start process 67.1183065\n",
      "object detected 67.1202552\n",
      "face detected 67.3990823\n",
      "face detected 67.6521468\n",
      "frame start process 67.8567731\n",
      "object detected 67.8582509\n",
      "face detected 68.1762488\n",
      "face detected 68.4270027\n",
      "frame start process 68.643203\n",
      "object detected 68.6446096\n",
      "face detected 69.0015554\n",
      "face detected 69.2491128\n",
      "frame start process 69.4531996\n",
      "object detected 69.4545584\n",
      "face detected 69.8053204\n",
      "frame start process 69.8344312\n",
      "object detected 69.835797\n",
      "face detected 70.1469054\n",
      "face detected 70.2008857\n",
      "frame start process 70.4037534\n",
      "object detected 70.40511\n",
      "face detected 70.7050165\n",
      "face detected 70.754694\n",
      "frame start process 70.9564455\n",
      "object detected 70.9577972\n",
      "face detected 71.2369274\n",
      "face detected 71.4859689\n",
      "frame start process 71.701189\n",
      "object detected 71.7028276\n",
      "face detected 71.9821925\n",
      "face detected 72.2366066\n",
      "frame start process 72.4693761\n",
      "object detected 72.4712515\n",
      "face detected 72.7642611\n",
      "face detected 73.043722\n",
      "frame start process 73.2585253\n",
      "object detected 73.2603921\n",
      "face detected 73.6116246\n",
      "face detected 73.912922\n",
      "frame start process 74.1311803\n",
      "object detected 74.1331992\n",
      "face detected 74.4468415\n",
      "face detected 74.7608524\n",
      "frame start process 75.002823\n",
      "object detected 75.0052898\n",
      "face detected 75.3490767\n",
      "face detected 75.6162019\n",
      "frame start process 75.8263683\n",
      "object detected 75.8278941\n",
      "face detected 76.098377\n",
      "face detected 76.3893809\n",
      "frame start process 76.6475447\n",
      "object detected 76.649755\n",
      "face detected 77.036812\n",
      "face detected 77.3584972\n",
      "frame start process 77.5841402\n",
      "object detected 77.5867101\n",
      "face detected 78.0123644\n",
      "face detected 78.2868339\n",
      "frame start process 78.4943247\n",
      "object detected 78.4958257\n",
      "face detected 78.7810724\n",
      "face detected 79.0359458\n",
      "frame start process 79.2520877\n",
      "object detected 79.2534706\n",
      "face detected 79.5450657\n",
      "face detected 79.7934581\n",
      "frame start process 80.005969\n",
      "object detected 80.0074116\n",
      "face detected 80.3026443\n",
      "face detected 80.5621464\n",
      "frame start process 80.7692438\n",
      "object detected 80.7708341\n",
      "face detected 81.0596917\n",
      "face detected 81.3244484\n",
      "frame start process 81.5305323\n",
      "object detected 81.5322595\n",
      "face detected 81.8519017\n",
      "face detected 82.10231\n",
      "frame start process 82.3184524\n",
      "object detected 82.3200883\n",
      "face detected 82.6064189\n",
      "face detected 82.8617416\n",
      "frame start process 83.0742023\n",
      "object detected 83.0754674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face detected 83.3625044\n",
      "face detected 83.6068821\n",
      "frame start process 83.8192627\n",
      "object detected 83.8211609\n",
      "face detected 84.0989904\n",
      "face detected 84.3449972\n",
      "frame start process 84.5520338\n",
      "object detected 84.5538203\n",
      "face detected 84.8294343\n",
      "face detected 85.0651449\n",
      "frame start process 85.2799742\n",
      "object detected 85.2815126\n",
      "face detected 85.565407\n",
      "face detected 85.8181997\n",
      "frame start process 86.0231165\n",
      "object detected 86.024995\n",
      "face detected 86.3086661\n",
      "face detected 86.5525645\n",
      "frame start process 86.7662879\n",
      "object detected 86.7676573\n",
      "face detected 87.0333039\n",
      "face detected 87.295152\n",
      "frame start process 87.5128639\n",
      "object detected 87.5144966\n",
      "face detected 87.8006847\n",
      "face detected 87.9960507\n",
      "frame start process 88.2160144\n",
      "object detected 88.2174157\n",
      "face detected 88.5108038\n",
      "face detected 88.7527792\n",
      "frame start process 88.97839\n",
      "object detected 88.9804233\n",
      "face detected 89.2674927\n",
      "face detected 89.5031472\n",
      "frame start process 89.710979\n",
      "object detected 89.7128149\n",
      "face detected 90.0054154\n",
      "face detected 90.2428572\n",
      "frame start process 90.4533894\n",
      "object detected 90.4548608\n",
      "face detected 90.7453295\n",
      "face detected 90.9767562\n",
      "frame start process 91.1936066\n",
      "object detected 91.1950141\n",
      "face detected 91.4896015\n",
      "face detected 91.7289898\n",
      "frame start process 91.9371023\n",
      "object detected 91.9386601\n",
      "face detected 92.2477034\n",
      "face detected 92.5227554\n",
      "frame start process 92.7447218\n",
      "object detected 92.7471737\n",
      "face detected 93.1560002\n",
      "face detected 93.4341816\n",
      "frame start process 93.6568198\n",
      "object detected 93.6587847\n",
      "face detected 94.0798976\n",
      "face detected 94.3300084\n",
      "face detected 94.542921\n",
      "frame start process 94.6269962\n",
      "object detected 94.628607\n",
      "face detected 94.9026713\n",
      "face detected 95.1314095\n",
      "frame start process 95.3434002\n",
      "object detected 95.3449565\n",
      "face detected 95.6406188\n",
      "face detected 95.8803159\n",
      "frame start process 96.1090809\n",
      "object detected 96.1110851\n",
      "face detected 96.4125156\n",
      "face detected 96.6515158\n",
      "frame start process 96.8612385\n",
      "object detected 96.862681\n",
      "face detected 97.1510081\n",
      "face detected 97.3984386\n",
      "frame start process 97.6028992\n",
      "object detected 97.6044485\n",
      "face detected 97.8906568\n",
      "face detected 98.1303665\n",
      "frame start process 98.3437897\n",
      "object detected 98.345195\n",
      "face detected 98.5800241\n",
      "face detected 98.8260174\n",
      "frame start process 99.0387864\n",
      "object detected 99.0401741\n",
      "face detected 99.3394537\n",
      "face detected 99.5824928\n",
      "frame start process 99.7967683\n",
      "object detected 99.7978211\n",
      "face detected 100.0899089\n",
      "face detected 100.3304341\n",
      "frame start process 100.5417394\n",
      "object detected 100.5429804\n",
      "face detected 100.8318877\n",
      "face detected 100.877114\n",
      "frame start process 101.081481\n",
      "object detected 101.0828198\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2747d9e90d0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m       \u001b[0mperson_coordinates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m     \u001b[0mclustering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold_distance\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperson_coordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m     \u001b[0misSafe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclustering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\envs\\social_mask\\lib\\site-packages\\sklearn\\cluster\\_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \"\"\"\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\envs\\social_mask\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\envs\\social_mask\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\envs\\social_mask\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "##################################### Analyze the Video ################################################\n",
    "\n",
    "# Load YOLOv3\n",
    "net = cv2.dnn.readNet(\"Models/\"+\"yolov3.weights\",\"Models/\"+\"yolov3.cfg\")\n",
    "# net = cv2.dnn.readNetFromCaffe(\"ssd/\"+\"MobileNet_deploy.prototxt\",\"ssd/\"+\"MobileNet_deploy.caffemodel\")\n",
    "\n",
    "# Load COCO Classes\n",
    "classes = []\n",
    "with open(\"Models/\"+\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Fetch Video Properties\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "width = int(320)\n",
    "height = int(200)\n",
    "n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "# Create Directory for Storing Results (Make sure it doesn't already exists !)\n",
    "\"\"\"\n",
    "os.mkdir(BASE_PATH+\"Results\")\n",
    "os.mkdir(BASE_PATH+\"Results/Extracted_Faces\")\n",
    "os.mkdir(BASE_PATH+\"Results/Extracted_Persons\")\n",
    "os.mkdir(BASE_PATH+\"Results/Frames\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Output Video Stream\n",
    "\"\"\"\n",
    "\n",
    "out_stream = cv2.VideoWriter(\n",
    "    'Results/Output.mp4',\n",
    "    cv2.VideoWriter_fourcc('X','V','I','D'),\n",
    "    fps,\n",
    "    (int(width),int(height)))\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "print(\"Processing Frames :\")\n",
    "while True:\n",
    "    print(\"frame start process\",time.perf_counter())\n",
    "    # Capture Frame-by-Frame\n",
    "    ret,img = cap.read()\n",
    "    # img = cv2.resize(img, (640, 480), interpolation = cv2.INTER_LINEAR) \n",
    "    \n",
    "    # Check EOF\n",
    "    if ret==False:\n",
    "        break;\n",
    "\n",
    "    # Get Frame Dimentions\n",
    "    height, width, channels = img.shape\n",
    "    \n",
    "    # Detect Objects in the Frame with YOLOv3\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (160, 160), (0, 0, 0), True, crop=False)\n",
    "    \n",
    "    print(\"object detected\",time.perf_counter())\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    \n",
    "    # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                \n",
    "                # Get Center, Height and Width of the Box\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Topleft Co-ordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Initialize empty lists for storing Bounding Boxes of People and their Faces\n",
    "    persons = []\n",
    "    masked_faces = []\n",
    "    unmasked_faces = []\n",
    "\n",
    "    # Work on Detected Persons in the Frame\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "\n",
    "            box = np.array(boxes[i])\n",
    "            box = np.where(box<0,0,box)\n",
    "            (x, y, w, h) = box\n",
    "\n",
    "            label = str(classes[class_ids[i]])\n",
    "\n",
    "            if label=='person':\n",
    "\n",
    "                persons.append([x,y,w,h])\n",
    "                \n",
    "                # Save Image of Cropped Person (If not required, comment the command below)\n",
    "                \"\"\"\n",
    "                \n",
    "                cv2.imwrite(BASE_PATH + \"Results/Extracted_Persons/\"+str(frame)\n",
    "                            +\"_\"+str(len(persons))+\".jpg\",\n",
    "                            img[y:y+h,x:x+w])\n",
    "                            \n",
    "                \"\"\"\n",
    "\n",
    "                # Detect Face in the Person\n",
    "                person_rgb = img[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n",
    "                detections = detector.detect(person_rgb)\n",
    "                print(\"face detected\",time.perf_counter())\n",
    "                # If a Face is Detected\n",
    "                if detections.shape[0] > 0:\n",
    "\n",
    "                  detection = np.array(detections[0])\n",
    "                  detection = np.where(detection<0,0,detection)\n",
    "\n",
    "                  # Calculating Co-ordinates of the Detected Face\n",
    "                  x1 = x + int(detection[0])\n",
    "                  x2 = x + int(detection[2])\n",
    "                  y1 = y + int(detection[1])\n",
    "                  y2 = y + int(detection[3])\n",
    "\n",
    "                  try :\n",
    "\n",
    "                    # Crop & BGR to RGB\n",
    "                    face_rgb = img[y1:y2,x1:x2,::-1]   \n",
    "\n",
    "                    # Preprocess the Image\n",
    "                    face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "                    face_arr = np.expand_dims(face_arr, axis=0)\n",
    "                    face_arr = preprocess_input(face_arr)\n",
    "\n",
    "                    # Predict if the Face is Masked or Not\n",
    "                    score = mask_classifier.predict(face_arr)\n",
    "\n",
    "                    # Determine and store Results\n",
    "                    if score[0][0]<0.5:\n",
    "                      masked_faces.append([x1,y1,x2,y2])\n",
    "                    else:\n",
    "                      unmasked_faces.append([x1,y1,x2,y2])\n",
    "\n",
    "                    # Save Image of Cropped Face (If not required, comment the command below)\n",
    "                    \"\"\"\n",
    "                    cv2.imwrite(BASE_PATH + \"Results/Extracted_Faces/\"+str(frame)\n",
    "                                +\"_\"+str(len(persons))+\".jpg\",\n",
    "                                img[y1:y2,x1:x2])\n",
    "                    \"\"\"\n",
    "\n",
    "                  except:\n",
    "                    continue\n",
    "    \n",
    "    # Calculate Coordinates of People Detected and find Clusters using DBSCAN\n",
    "    person_coordinates = []\n",
    "\n",
    "    for p in range(len(persons)):\n",
    "      person_coordinates.append((persons[p][0]+int(persons[p][2]/2),persons[p][1]+int(persons[p][3]/2)))\n",
    "\n",
    "    clustering = DBSCAN(eps=threshold_distance,min_samples=2).fit(person_coordinates)\n",
    "    isSafe = clustering.labels_\n",
    "\n",
    "    # Count \n",
    "    person_count = len(persons)\n",
    "    masked_face_count = len(masked_faces)\n",
    "    unmasked_face_count = len(unmasked_faces)\n",
    "    safe_count = np.sum((isSafe==-1)*1)\n",
    "    unsafe_count = person_count - safe_count\n",
    "\n",
    "    # Show Clusters using Red Lines\n",
    "    arg_sorted = np.argsort(isSafe)\n",
    "\n",
    "    for i in range(1,person_count):\n",
    "\n",
    "      if isSafe[arg_sorted[i]]!=-1 and isSafe[arg_sorted[i]]==isSafe[arg_sorted[i-1]]:\n",
    "        cv2.line(img,person_coordinates[arg_sorted[i]],person_coordinates[arg_sorted[i-1]],(0,0,255),2)\n",
    "\n",
    "    # Put Bounding Boxes on People in the Frame\n",
    "    for p in range(person_count):\n",
    "\n",
    "      a,b,c,d = persons[p]\n",
    "\n",
    "      # Green if Safe, Red if UnSafe\n",
    "      if isSafe[p]==-1:\n",
    "        cv2.rectangle(img, (a, b), (a + c, b + d), (0,255,0), 2)\n",
    "      else:\n",
    "        cv2.rectangle(img, (a, b), (a + c, b + d), (0,0,255), 2)\n",
    "\n",
    "    # Put Bounding Boxes on Faces in the Frame\n",
    "    # Green if Safe, Red if UnSafe\n",
    "    for f in range(masked_face_count):\n",
    "\n",
    "      a,b,c,d = masked_faces[f]\n",
    "      cv2.rectangle(img, (a, b), (c,d), (0,255,0), 2)\n",
    "\n",
    "    for f in range(unmasked_face_count):\n",
    "\n",
    "      a,b,c,d = unmasked_faces[f]\n",
    "      cv2.rectangle(img, (a, b), (c,d), (0,0,255), 2)\n",
    "\n",
    "    # Show Monitoring Status in a Black Box at the Top\n",
    "    cv2.rectangle(img,(0,0),(width,50),(0,0,0),-1)\n",
    "    cv2.rectangle(img,(1,1),(width-1,50),(255,255,255),2)\n",
    "\n",
    "    xpos = 15\n",
    "\n",
    "    string = \"Total People = \"+str(person_count)\n",
    "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "\n",
    "    string = \" ( \"+str(safe_count) + \" Safe \"\n",
    "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "\n",
    "    string = str(unsafe_count)+ \" Unsafe ) \"\n",
    "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "    \n",
    "    string = \"( \" +str(masked_face_count)+\" Masked \"+str(unmasked_face_count)+\" Unmasked \"+\\\n",
    "             str(person_count-masked_face_count-unmasked_face_count)+\" Unknown )\"\n",
    "    cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n",
    "\n",
    "    # Write Frame to the Output File\n",
    "    # out_stream.write(img)\n",
    "\n",
    "    # Save the Frame in frame_no.png format (If not required, comment the command below)\n",
    "    \"\"\"\n",
    "    cv2.imwrite(BASE_PATH+\"Results/Frames/\"+str(frame)+\".jpg\",img)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Use if you want to see Results Frame by Frame\n",
    "    cv2.imshow(\"Street Cam\",img)\n",
    "    # output_movie.write(img)\n",
    "\n",
    "    # Exit on Pressing Q Key\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release Streams\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Good to Go!\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BDtRjQqyEbp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Social_Distancing_Monitor1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2fcfd62efc6b4cd1b0dd14b646254afe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_874d2f0343564a238d15b9d61d3bdeaf",
       "IPY_MODEL_8910c4e06c0346b187525153761f2657"
      ],
      "layout": "IPY_MODEL_3c354ebd36324219bdcdf98c96b72721"
     }
    },
    "3c354ebd36324219bdcdf98c96b72721": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "874d2f0343564a238d15b9d61d3bdeaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b33a91cd105f489abb9294f003f2ccbb",
      "max": 481004605,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a188a817c2bd43c5aa85e4ea7c848c66",
      "value": 481004605
     }
    },
    "8910c4e06c0346b187525153761f2657": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8acf0e98afb432b9bc3665dcb41dd98",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_edf7785d6c47473e95b8066c025de957",
      "value": " 459M/459M [00:39&lt;00:00, 12.3MB/s]"
     }
    },
    "a188a817c2bd43c5aa85e4ea7c848c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b33a91cd105f489abb9294f003f2ccbb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf7785d6c47473e95b8066c025de957": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8acf0e98afb432b9bc3665dcb41dd98": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
