{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgpQJHNSfsK6"
   },
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import face_detection \n",
    "from sklearn.cluster import DBSCAN\n",
    "from keras.models import load_model\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import tqdm\n",
    "import argparse\n",
    "# from imutils.video import FileVideoStream\n",
    "# from imutils.video import FPS\n",
    "# import imutils\n",
    "import time\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport cv2\\nimport face_detection\\nprint(face_detection.available_detectors)\\ndetector = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.5, nms_iou_threshold=.3)\\nmask_classifier = load_model(\"Models/mask_detector.model\")\\n\\ncap = cv2.VideoCapture(0)\\nc=0\\n\\nwhile True:\\n    c+=1\\n    # BGR to RGB\\n    ret = cap.grab()\\n        \\n    if c%5==0:\\n        \\n        ret,img = cap.retrieve()\\n\\n        if ret==False:\\n            break;\\n\\n\\n\\n        # resize image\\n        # img = cv2.resize(img, (720, 720), interpolation = cv2.INTER_AREA)\\n\\n\\n        im=img[:, :, ::-1]\\n        detections = detector.detect(im)\\n\\n        for face in detections:\\n            x1 =int(face[0])\\n            x2 =int(face[2])\\n            y1 =int(face[1])\\n            y2 =int(face[3])\\n            \\n            face_rgb = img[y1:y2,x1:x2,::-1]   \\n            \\n            # Preprocess the Image\\n            face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\\n            face_arr = np.expand_dims(face_arr, axis=0)\\n            face_arr = preprocess_input(face_arr)\\n            # Predict if the Face is Masked or Not\\n            score = mask_classifier.predict(face_arr)\\n\\n            if score[0][0]<0.2:\\n                cv2.rectangle(img, (x1, y1), (x2,y2), (0,0,255), 2)\\n                print(\"safe\")\\n            else:\\n                cv2.rectangle(img, (x1, y1), (x2,y2), (0,255,0), 2)\\n                print(\"unsafe\")\\n            \\n\\n        cv2.imshow(\"Cam\",img)\\n        # output_movie.write(img)\\n    if cv2.waitKey(25) & 0xFF == ord(\\'q\\'):\\n        break;\\n# Release Streams\\ncap.release()\\ncv2.destroyAllWindows()\\n\\n# Good to Go!\\nprint(\"Done !\")\\n\\n\\n# Experiment 3\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "import cv2\n",
    "import face_detection\n",
    "print(face_detection.available_detectors)\n",
    "detector = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "mask_classifier = load_model(\"Models/mask_detector.model\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "c=0\n",
    "\n",
    "while True:\n",
    "    c+=1\n",
    "    # BGR to RGB\n",
    "    ret = cap.grab()\n",
    "        \n",
    "    if c%5==0:\n",
    "        \n",
    "        ret,img = cap.retrieve()\n",
    "\n",
    "        if ret==False:\n",
    "            break;\n",
    "\n",
    "\n",
    "\n",
    "        # resize image\n",
    "        # img = cv2.resize(img, (720, 720), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "        im=img[:, :, ::-1]\n",
    "        detections = detector.detect(im)\n",
    "\n",
    "        for face in detections:\n",
    "            x1 =int(face[0])\n",
    "            x2 =int(face[2])\n",
    "            y1 =int(face[1])\n",
    "            y2 =int(face[3])\n",
    "            \n",
    "            face_rgb = img[y1:y2,x1:x2,::-1]   \n",
    "            \n",
    "            # Preprocess the Image\n",
    "            face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "            face_arr = np.expand_dims(face_arr, axis=0)\n",
    "            face_arr = preprocess_input(face_arr)\n",
    "            # Predict if the Face is Masked or Not\n",
    "            score = mask_classifier.predict(face_arr)\n",
    "\n",
    "            if score[0][0]<0.2:\n",
    "                cv2.rectangle(img, (x1, y1), (x2,y2), (0,0,255), 2)\n",
    "                print(\"safe\")\n",
    "            else:\n",
    "                cv2.rectangle(img, (x1, y1), (x2,y2), (0,255,0), 2)\n",
    "                print(\"unsafe\")\n",
    "            \n",
    "\n",
    "        cv2.imshow(\"Cam\",img)\n",
    "        # output_movie.write(img)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break;\n",
    "# Release Streams\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Good to Go!\n",
    "print(\"Done !\")\n",
    "\n",
    "\n",
    "# Experiment 3\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygki7lR_sFvf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparser = argparse.ArgumentParser(description=\"SocialDistancing\")\\nparser.add_argument(\"--videopath\", type=str, default=\"video1.mp4\", help=\"Path to the video file\")\\nargs = parser.parse_args()\\n\\nFILE_PATH = args.videopath\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the Working Environment\n",
    "\n",
    "# If using Google Colab (If on a Local Environment, no path required => set BASE_PATH  = \"\")\n",
    "BASE_PATH = \"drive/My Drive/Social_Distancing_with_AI-master/\"\n",
    "\n",
    "# Path to Input Video File in the BASE_PATH\n",
    "FILE_PATH = \"video1.mp4\"\n",
    "# Command-line input setup\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(description=\"SocialDistancing\")\n",
    "parser.add_argument(\"--videopath\", type=str, default=\"video1.mp4\", help=\"Path to the video file\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "FILE_PATH = args.videopath\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "2fcfd62efc6b4cd1b0dd14b646254afe",
      "3c354ebd36324219bdcdf98c96b72721",
      "874d2f0343564a238d15b9d61d3bdeaf",
      "8910c4e06c0346b187525153761f2657",
      "a188a817c2bd43c5aa85e4ea7c848c66",
      "b33a91cd105f489abb9294f003f2ccbb",
      "edf7785d6c47473e95b8066c025de957",
      "f8acf0e98afb432b9bc3665dcb41dd98"
     ]
    },
    "colab_type": "code",
    "id": "2xrRSzoT9EBa",
    "outputId": "e6ae9db2-a205-459e-a437-495c11bd1616"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DSFDDetector', 'RetinaNetResNet50', 'RetinaNetMobileNetV1']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Face Detector \n",
    "# Confidence Threshold can be Adjusted, Greater values would Detect only Clear Faces\n",
    "\n",
    "print(face_detection.available_detectors)\n",
    "detector = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.5, nms_iou_threshold=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kv-5woacC0C5"
   },
   "outputs": [],
   "source": [
    "# Load Pretrained Face Mask Classfier (Keras Model)\n",
    "\n",
    "mask_classifier = load_model(\"Models/mask_detector.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8O6mF_orCxN4"
   },
   "outputs": [],
   "source": [
    "# Set the Safe Distance in Pixel Units (Minimum Distance Expected to be Maintained between People)\n",
    "# This Parameter would Affect the Results, Adjust according to the Footage captured by CCTV Camera \n",
    "\n",
    "threshold_distance = 150  # Try with different Values before Finalizing\n",
    "mask_detector_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ejHxuy-9iZwL",
    "outputId": "3091f973-51fc-44b8-8972-c57a792772c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Frames :\n",
      "frame start process 531.4052756\n",
      "object detected 531.4067991\n",
      "frame start process 532.1108354\n",
      "object detected 532.1126363\n",
      "frame start process 532.5500097\n",
      "object detected 532.5520721\n",
      "frame start process 532.950553\n",
      "object detected 532.9525082\n",
      "frame start process 533.3517479\n",
      "object detected 533.3536186\n",
      "frame start process 533.7303967\n",
      "object detected 533.7322616\n",
      "frame start process 534.1308718\n",
      "object detected 534.1327563\n",
      "frame start process 534.5037298\n",
      "object detected 534.5053102\n",
      "frame start process 534.9171124\n",
      "object detected 534.9189715\n",
      "frame start process 535.3559365\n",
      "object detected 535.3577579\n",
      "face detected 535.6739939\n",
      "face classified 535.7460104\n",
      "frame start process 535.9156138\n",
      "object detected 535.9175373\n",
      "face detected 536.1851173\n",
      "face classified 536.2564731\n",
      "frame start process 536.3578191\n",
      "object detected 536.3600236\n",
      "face detected 536.6456478\n",
      "face classified 536.7149516\n",
      "frame start process 536.8210579\n",
      "object detected 536.8224411\n",
      "face detected 537.1239584\n",
      "face classified 537.1906839\n",
      "frame start process 537.2221459\n",
      "object detected 537.2241005\n",
      "face detected 537.5055449\n",
      "face classified 537.5759665\n",
      "frame start process 537.6930986\n",
      "object detected 537.6948423\n",
      "face detected 538.0011779\n",
      "face classified 538.0698524\n",
      "frame start process 538.1012866\n",
      "object detected 538.1028988\n",
      "face detected 538.3902458\n",
      "face classified 538.4597146\n",
      "frame start process 538.6318375\n",
      "object detected 538.6332071\n",
      "face detected 538.927865\n",
      "face classified 538.9999298\n",
      "frame start process 539.0860379\n",
      "object detected 539.0876671\n",
      "face detected 539.386232\n",
      "face classified 539.4595628\n",
      "frame start process 539.4964734\n",
      "object detected 539.4979519\n",
      "face detected 539.8249299\n",
      "face classified 539.9164005\n",
      "frame start process 539.9483586\n",
      "object detected 539.9500381\n",
      "face detected 540.2874383\n",
      "face classified 540.3706813\n",
      "frame start process 540.4489971\n",
      "object detected 540.4506323\n",
      "face detected 540.7432899\n",
      "face classified 540.8326275\n",
      "frame start process 540.9538072\n",
      "object detected 540.9551959\n",
      "face detected 541.2490264\n",
      "face classified 541.319815\n",
      "frame start process 541.388886\n",
      "object detected 541.3904924\n",
      "face detected 541.6930121\n",
      "face classified 541.7639901\n",
      "frame start process 541.8441083\n",
      "object detected 541.8457567\n",
      "face detected 542.1122322\n",
      "frame start process 542.1806038\n",
      "object detected 542.1823376\n",
      "frame start process 542.6181399\n",
      "object detected 542.6200615\n",
      "face detected 542.8709736\n",
      "frame start process 542.9360927\n",
      "object detected 542.9377326\n",
      "face detected 543.1963261\n",
      "frame start process 543.3529047\n",
      "object detected 543.3544943\n",
      "face detected 543.6006611\n",
      "frame start process 543.7197827\n",
      "object detected 543.7213933\n",
      "face detected 543.9771164\n",
      "frame start process 544.0509472\n",
      "object detected 544.0525301\n",
      "face detected 544.3127601\n",
      "frame start process 544.4237846\n",
      "object detected 544.4254166\n",
      "face detected 544.7750479\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "##################################### Analyze the Video ################################################\n",
    "\n",
    "# Load YOLOv3\n",
    "net = cv2.dnn.readNet(\"Models/\"+\"yolov4.weights\",\"Models/\"+\"yolov4.cfg\")\n",
    "# net = cv2.dnn.readNetFromCaffe(\"ssd/\"+\"MobileNet_deploy.prototxt\",\"ssd/\"+\"MobileNet_deploy.caffemodel\")\n",
    "\n",
    "# Load COCO Classes\n",
    "classes = []\n",
    "with open(\"Models/\"+\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Fetch Video Properties\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# n_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "# Create Directory for Storing Results (Make sure it doesn't already exists !)\n",
    "\"\"\"\n",
    "os.mkdir(BASE_PATH+\"Results\")\n",
    "os.mkdir(BASE_PATH+\"Results/Extracted_Faces\")\n",
    "os.mkdir(BASE_PATH+\"Results/Extracted_Persons\")\n",
    "os.mkdir(BASE_PATH+\"Results/Frames\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Output Video Stream\n",
    "\"\"\"\n",
    "\n",
    "out_stream = cv2.VideoWriter(\n",
    "    'Results/Output.mp4',\n",
    "    cv2.VideoWriter_fourcc('X','V','I','D'),\n",
    "    fps,\n",
    "    (int(width),int(height)))\n",
    "    \n",
    "\"\"\"\n",
    "c=0\n",
    "print(\"Processing Frames :\")\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret = cap.grab()\n",
    "    c+=1\n",
    "    \n",
    "    if c%5==0:\n",
    "        print(\"frame start process\",time.perf_counter())\n",
    "        # Capture Frame-by-Frame\n",
    "        ret,img = cap.retrieve()\n",
    "        img = cv2.resize(img, (640, 480), interpolation = cv2.INTER_LINEAR) \n",
    "\n",
    "        # Check EOF\n",
    "        if ret==False:\n",
    "            break;\n",
    "\n",
    "        # Get Frame Dimentions\n",
    "        height, width, channels = img.shape\n",
    "        # Detect Objects in the Frame with YOLOv3\n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (160, 160), (0, 0, 0), True, crop=False)\n",
    "\n",
    "        print(\"object detected\",time.perf_counter())\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        # Store Detected Objects with Labels, Bounding_Boxes and their Confidences\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "\n",
    "                    # Get Center, Height and Width of the Box\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Topleft Co-ordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Initialize empty lists for storing Bounding Boxes of People and their Faces\n",
    "        persons = []\n",
    "        masked_faces = []\n",
    "        unmasked_faces = []\n",
    "\n",
    "        # Work on Detected Persons in the Frame\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "\n",
    "                box = np.array(boxes[i])\n",
    "                box = np.where(box<0,0,box)\n",
    "                (x, y, w, h) = box\n",
    "\n",
    "                label = str(classes[class_ids[i]])\n",
    "\n",
    "                if label=='person':\n",
    "\n",
    "                    persons.append([x,y,w,h])\n",
    "\n",
    "                    # Save Image of Cropped Person (If not required, comment the command below)\n",
    "                    \"\"\"\n",
    "\n",
    "                    cv2.imwrite(BASE_PATH + \"Results/Extracted_Persons/\"+str(frame)\n",
    "                                +\"_\"+str(len(persons))+\".jpg\",\n",
    "                                img[y:y+h,x:x+w])\n",
    "\n",
    "                    \"\"\"\n",
    "\n",
    "                    # Detect Face in the Person\n",
    "                    person_rgb = img[y:y+h,x:x+w,::-1]   # Crop & BGR to RGB\n",
    "                    detections = detector.detect(person_rgb)\n",
    "\n",
    "                    print(\"face detected\",time.perf_counter())\n",
    "\n",
    "                    # If a Face is Detected\n",
    "                    if detections.shape[0] > 0:\n",
    "\n",
    "                      detection = np.array(detections[0])\n",
    "                      detection = np.where(detection<0,0,detection)\n",
    "\n",
    "                      # Calculating Co-ordinates of the Detected Face\n",
    "                      x1 = x + int(detection[0])\n",
    "                      x2 = x + int(detection[2])\n",
    "                      y1 = y + int(detection[1])\n",
    "                      y2 = y + int(detection[3])\n",
    "\n",
    "                      try :\n",
    "\n",
    "                        # Crop & BGR to RGB\n",
    "                        face_rgb = img[y1:y2,x1:x2,::-1]   \n",
    "\n",
    "                        # Preprocess the Image\n",
    "                        face_arr = cv2.resize(face_rgb, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "                        face_arr = np.expand_dims(face_arr, axis=0)\n",
    "                        face_arr = preprocess_input(face_arr)\n",
    "\n",
    "                        # Predict if the Face is Masked or Not\n",
    "                        score = mask_classifier.predict(face_arr)\n",
    "\n",
    "                        print(\"face classified\",time.perf_counter())\n",
    "\n",
    "\n",
    "                        # Determine and store Results\n",
    "                        if score[0][0]<mask_detector_threshold:\n",
    "                          unmasked_faces.append([x1,y1,x2,y2])\n",
    "                        else:\n",
    "                          masked_faces.append([x1,y1,x2,y2])\n",
    "\n",
    "                        # Save Image of Cropped Face (If not required, comment the command below)\n",
    "                        \"\"\"\n",
    "                        cv2.imwrite(BASE_PATH + \"Results/Extracted_Faces/\"+str(frame)\n",
    "                                    +\"_\"+str(len(persons))+\".jpg\",\n",
    "                                    img[y1:y2,x1:x2])\n",
    "                        \"\"\"\n",
    "\n",
    "                      except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "        # Calculate Coordinates of People Detected and find Clusters using DBSCAN\n",
    "        person_coordinates = []\n",
    "\n",
    "        for p in range(len(persons)):\n",
    "          person_coordinates.append((persons[p][0]+int(persons[p][2]/2),persons[p][1]+int(persons[p][3]/2)))\n",
    "\n",
    "        \n",
    "        try:\n",
    "            clustering = DBSCAN(eps=threshold_distance,min_samples=2).fit(person_coordinates)\n",
    "            isSafe = clustering.labels_\n",
    "        except:\n",
    "            isSafe=0    \n",
    "        \n",
    "\n",
    "        # Count \n",
    "        person_count = len(persons)\n",
    "        masked_face_count = len(masked_faces)\n",
    "        unmasked_face_count = len(unmasked_faces)\n",
    "        safe_count = np.sum((isSafe==-1)*1)\n",
    "        unsafe_count = person_count - safe_count\n",
    "\n",
    "        # Show Clusters using Red Lines\n",
    "        arg_sorted = np.argsort(isSafe)\n",
    "\n",
    "        for i in range(1,person_count):\n",
    "\n",
    "          if isSafe[arg_sorted[i]]!=-1 and isSafe[arg_sorted[i]]==isSafe[arg_sorted[i-1]]:\n",
    "            cv2.line(img,person_coordinates[arg_sorted[i]],person_coordinates[arg_sorted[i-1]],(0,0,255),2)\n",
    "\n",
    "        # Put Bounding Boxes on People in the Frame\n",
    "        for p in range(person_count):\n",
    "\n",
    "          a,b,c,d = persons[p]\n",
    "\n",
    "          # Green if Safe, Red if UnSafe\n",
    "          if isSafe[p]==-1:\n",
    "            cv2.rectangle(img, (a, b), (a + c, b + d), (0,255,0), 2)\n",
    "          else:\n",
    "            cv2.rectangle(img, (a, b), (a + c, b + d), (0,0,255), 2)\n",
    "\n",
    "        # Put Bounding Boxes on Faces in the Frame\n",
    "        # Green if Safe, Red if UnSafe\n",
    "        for f in range(masked_face_count):\n",
    "\n",
    "          a,b,c,d = masked_faces[f]\n",
    "          cv2.rectangle(img, (a, b), (c,d), (0,255,0), 2)\n",
    "\n",
    "        for f in range(unmasked_face_count):\n",
    "\n",
    "          a,b,c,d = unmasked_faces[f]\n",
    "          cv2.rectangle(img, (a, b), (c,d), (0,0,255), 2)\n",
    "\n",
    "        # Show Monitoring Status in a Black Box at the Top\n",
    "        cv2.rectangle(img,(0,0),(width,50),(0,0,0),-1)\n",
    "        cv2.rectangle(img,(1,1),(width-1,50),(255,255,255),2)\n",
    "\n",
    "        xpos = 15\n",
    "\n",
    "        string = \"Total People = \"+str(person_count)\n",
    "        cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "        xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "\n",
    "        string = \" ( \"+str(safe_count) + \" Safe \"\n",
    "        cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "\n",
    "        string = str(unsafe_count)+ \" Unsafe ) \"\n",
    "        cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "        xpos += cv2.getTextSize(string,cv2.FONT_HERSHEY_SIMPLEX,1,2)[0][0]\n",
    "\n",
    "        string = \"( \" +str(masked_face_count)+\" Masked \"+str(unmasked_face_count)+\" Unmasked \"+\\\n",
    "                 str(person_count-masked_face_count-unmasked_face_count)+\" Unknown )\"\n",
    "        cv2.putText(img,string,(xpos,35),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,255),2)\n",
    "\n",
    "        # Write Frame to the Output File\n",
    "        # out_stream.write(img)\n",
    "\n",
    "        # Save the Frame in frame_no.png format (If not required, comment the command below)\n",
    "        \"\"\"\n",
    "        cv2.imwrite(BASE_PATH+\"Results/Frames/\"+str(frame)+\".jpg\",img)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Use if you want to see Results Frame by Frame\n",
    "        cv2.imshow(\"Street Cam\",img)\n",
    "        # output_movie.write(img)\n",
    "\n",
    "    # Exit on Pressing Q Key\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release Streams\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Good to Go!\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BDtRjQqyEbp"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-51c4ed789b8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Social_Distancing_Monitor1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2fcfd62efc6b4cd1b0dd14b646254afe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_874d2f0343564a238d15b9d61d3bdeaf",
       "IPY_MODEL_8910c4e06c0346b187525153761f2657"
      ],
      "layout": "IPY_MODEL_3c354ebd36324219bdcdf98c96b72721"
     }
    },
    "3c354ebd36324219bdcdf98c96b72721": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "874d2f0343564a238d15b9d61d3bdeaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b33a91cd105f489abb9294f003f2ccbb",
      "max": 481004605,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a188a817c2bd43c5aa85e4ea7c848c66",
      "value": 481004605
     }
    },
    "8910c4e06c0346b187525153761f2657": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8acf0e98afb432b9bc3665dcb41dd98",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_edf7785d6c47473e95b8066c025de957",
      "value": " 459M/459M [00:39&lt;00:00, 12.3MB/s]"
     }
    },
    "a188a817c2bd43c5aa85e4ea7c848c66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b33a91cd105f489abb9294f003f2ccbb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf7785d6c47473e95b8066c025de957": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8acf0e98afb432b9bc3665dcb41dd98": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
